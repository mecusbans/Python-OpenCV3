{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYBz5471rzb1sY/Yxq6zPu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mecusbans/Python-OpenCV3/blob/master/location.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwfPxXaiBvOk",
        "outputId": "9e771850-2607-479b-a82d-57f05f61e84d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# Append the directory to your python path using sys\n",
        "sys.path.append('/content/drive/MyDrive/STTF-Rec/')"
      ],
      "metadata": {
        "id": "W0xHzI3TBzRd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from load import *\n",
        "import time\n",
        "import random\n",
        "from torch import optim\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "from models import *"
      ],
      "metadata": {
        "id": "0Jmgsm4SKn16"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def calculate_acc(prob, label, f2):#acc计算\n",
        "    # log_prob (N, L), label (N), batch_size [*M]\n",
        "    acc_train = [0, 0, 0, 0]\n",
        "    for i, k in enumerate([1, 5, 10, 20]):\n",
        "        # topk_batch (N, k)\n",
        "        _, topk_predict_batch = torch.topk(prob, k=k)\n",
        "        for j, topk_predict in enumerate(to_npy(topk_predict_batch)):\n",
        "            # topk_predict (k)\n",
        "            if to_npy(label)[j] in topk_predict:\n",
        "                acc_train[i] += 1\n",
        "            if k == 20:\n",
        "                f2.write('next loc:{}'.format(label[j]))\n",
        "                f2.write('rec loc:{}'.format(topk_predict_batch))\n",
        "                f2.write('\\n')\n",
        "\n",
        "    return np.array(acc_train)\n",
        "\n",
        "\n",
        "def sampling_prob(prob, label, num_neg):\n",
        "    num_label, l_m = prob.shape[0], prob.shape[1]-1  # prob (N, L)\n",
        "    label = label.view(-1)  # label (N)\n",
        "    init_label = np.linspace(0, num_label-1, num_label)  # (N), [0 -- num_label-1]\n",
        "    init_prob = torch.zeros(size=(num_label, num_neg+len(label)))  # (N, num_neg+num_label)\n",
        "\n",
        "    random_ig = random.sample(range(1, l_m+1), num_neg)  # (num_neg) from (1 -- l_max)\n",
        "    while len([lab for lab in label if lab in random_ig]) != 0:  # no intersection\n",
        "        random_ig = random.sample(range(1, l_m+1), num_neg)\n",
        "\n",
        "    global global_seed\n",
        "    random.seed(global_seed)\n",
        "    global_seed += 1\n",
        "\n",
        "    # place the pos labels ahead and neg samples in the end\n",
        "    for k in range(num_label):\n",
        "        for i in range(num_neg + len(label)):\n",
        "            if i < len(label):\n",
        "                init_prob[k, i] = prob[k, label[i]]\n",
        "            else:\n",
        "                init_prob[k, i] = prob[k, random_ig[i-len(label)]]\n",
        "\n",
        "    return torch.FloatTensor(init_prob), torch.LongTensor(init_label)  # (N, num_neg+num_label), (N)\n",
        "\n",
        "\n",
        "class DataSet(data.Dataset):#数据集\n",
        "    def __init__(self, traj, m1, v, label, length):\n",
        "        # (NUM, M, 3), (NUM, M, M, 2), (L, L), (NUM, M), (NUM), (NUM)\n",
        "        self.traj, self.mat1, self.vec, self.label, self.length = traj, m1, v, label, length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        traj = self.traj[index].to(device)#拷贝到GPU\n",
        "        mats1 = self.mat1[index].to(device)\n",
        "        vector = self.vec[index].to(device)\n",
        "        label = self.label[index].to(device)\n",
        "        length = self.length[index].to(device)\n",
        "        return traj, mats1, vector, label, length\n",
        "\n",
        "    def __len__(self):  # no use\n",
        "        return len(self.traj)\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, record):\n",
        "        # load other parameters\n",
        "        self.model = model.to(device)\n",
        "        self.records = record\n",
        "        self.start_epoch = record['epoch'][-1] if load else 1\n",
        "        self.num_neg = 10\n",
        "        self.interval = 1000\n",
        "        self.batch_size = 1 # N = 1\n",
        "        self.learning_rate = 3e-3\n",
        "        self.num_epoch = 5\n",
        "        self.threshold = np.mean(record['acc_valid'][-1]) if load else 0  # 0 if not update\n",
        "\n",
        "        # (NUM, M, 3), (NUM, M, M, 2), (L, L), (NUM, M, M), (NUM, M), (NUM) i.e. [*M]\n",
        "        self.traj, self.mat1, self.mat2s, self.mat2t, self.label, self.len = \\\n",
        "            trajs, mat1, mat2s, mat2t, labels, lens\n",
        "        # nn.cross_entropy_loss counts target from 0 to C - 1, so we minus 1 here.\n",
        "        self.dataset = DataSet(self.traj, self.mat1, self.mat2t, self.label-1, self.len)\n",
        "        self.data_loader = data.DataLoader(dataset=self.dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def train(self):\n",
        "        # set optimizer 优化器\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=0)#lr学习率（步长）weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=1)#调整学习率\n",
        "        f=open(\"result.txt\",\"w\",encoding='utf-8')\n",
        "        f2 = open(\"rec_point.txt\", \"w\", encoding='utf-8')\n",
        "        for t in range(self.num_epoch):\n",
        "            # settings or validation and test\n",
        "            valid_size, test_size = 0, 0\n",
        "            acc_valid, acc_test = [0, 0, 0, 0], [0, 0, 0, 0]\n",
        "\n",
        "            bar = tqdm(total=part)\n",
        "            for step, item in enumerate(self.data_loader):\n",
        "                # get batch data, (N, M, 3), (N, M, M, 2), (N, M, M), (N, M), (N)\n",
        "                person_input, person_m1, person_m2t, person_label, person_traj_len = item\n",
        "\n",
        "                # first, try batch_size = 1 and mini_batch = 1\n",
        "\n",
        "                input_mask = torch.zeros((self.batch_size, max_len, 3), dtype=torch.long).to(device)\n",
        "                m1_mask = torch.zeros((self.batch_size, max_len, max_len, 2), dtype=torch.float32).to(device)\n",
        "                for mask_len in range(1, person_traj_len[0]+1):  # from 1 -> len\n",
        "                    # if mask_len != person_traj_len[0]:\n",
        "                    #     continue\n",
        "                    input_mask[:, :mask_len] = 1.\n",
        "                    m1_mask[:, :mask_len, :mask_len] = 1.\n",
        "\n",
        "                    train_input = person_input * input_mask\n",
        "                    train_m1 = person_m1 * m1_mask\n",
        "                    train_m2t = person_m2t[:, mask_len - 1]\n",
        "                    train_label = person_label[:, mask_len - 1]  # (N)\n",
        "                    train_len = torch.zeros(size=(self.batch_size,), dtype=torch.long).to(device) + mask_len\n",
        "\n",
        "                    prob = self.model(train_input, train_m1, self.mat2s, train_m2t, train_len)  # (N, L)!!!\n",
        "\n",
        "                    if mask_len <= person_traj_len[0] - 2:  # only training\n",
        "                        # nn.utils.clip_grad_norm_(self.model.parameters(), 10)\n",
        "                        prob_sample, label_sample = sampling_prob(prob, train_label, self.num_neg)\n",
        "                        loss_train = F.cross_entropy(prob_sample, label_sample)\n",
        "                        loss_train.backward()\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "                        scheduler.step()\n",
        "\n",
        "                    elif mask_len == person_traj_len[0] - 1:  # only validation\n",
        "                        valid_size += person_input.shape[0]\n",
        "                        # v_prob_sample, v_label_sample = sampling_prob(prob_valid, valid_label, self.num_neg)\n",
        "                        # loss_valid += F.cross_entropy(v_prob_sample, v_label_sample, reduction='sum')\n",
        "                        acc_valid += calculate_acc(prob, train_label, f2)\n",
        "\n",
        "                    elif mask_len == person_traj_len[0]:  # only test\n",
        "                        test_size += person_input.shape[0]\n",
        "                        # v_prob_sample, v_label_sample = sampling_prob(prob_valid, valid_label, self.num_neg)\n",
        "                        # loss_valid += F.cross_entropy(v_prob_sample, v_label_sample, reduction='sum')\n",
        "                        acc_test += calculate_acc(prob, train_label, f2)\n",
        "\n",
        "                bar.update(self.batch_size)\n",
        "            bar.close()\n",
        "\n",
        "            acc_valid = np.array(acc_valid) / valid_size\n",
        "            print('epoch:{}, time:{}, valid_acc:{}'.format(self.start_epoch + t, time.time() - start, acc_valid))\n",
        "            # with open(\"result.txt\",\"w\") as f:\n",
        "            f.write('epoch:{}, time:{}, valid_acc:{}'.format(self.start_epoch + t, time.time() - start, acc_valid))\n",
        "            f.write('\\n')\n",
        "\n",
        "            acc_test = np.array(acc_test) / test_size\n",
        "            print('epoch:{}, time:{}, test_acc:{}'.format(self.start_epoch + t, time.time() - start, acc_test))\n",
        "            # with open(\"result.txt\", \"w\") as f:\n",
        "            f.write('epoch:{}, time:{}, test_acc:{}'.format(self.start_epoch + t, time.time() - start, acc_test))\n",
        "            f.write('\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            self.records['acc_valid'].append(acc_valid)\n",
        "            self.records['acc_test'].append(acc_test)\n",
        "            self.records['epoch'].append(self.start_epoch + t)\n",
        "\n",
        "            if self.threshold < np.mean(acc_valid):\n",
        "                self.threshold = np.mean(acc_valid)\n",
        "                # save the model\n",
        "                torch.save({'state_dict': self.model.state_dict(),\n",
        "                            'records': self.records,\n",
        "                            'time': time.time() - start},\n",
        "                           'best_stan_win_1000_' + dname + '.pth')\n",
        "        f.close()\n",
        "\n",
        "    def inference(self):\n",
        "        user_ids = []\n",
        "        for t in range(self.num_epoch):\n",
        "            # settings or validation and test\n",
        "            valid_size, test_size = 0, 0\n",
        "            acc_valid, acc_test = [0, 0, 0, 0], [0, 0, 0, 0]\n",
        "            cum_valid, cum_test = [0, 0, 0, 0], [0, 0, 0, 0]\n",
        "\n",
        "            for step, item in enumerate(self.data_loader):\n",
        "                # get batch data, (N, M, 3), (N, M, M, 2), (N, M, M), (N, M), (N)\n",
        "                person_input, person_m1, person_m2t, person_label, person_traj_len = item\n",
        "\n",
        "                # first, try batch_size = 1 and mini_batch = 1\n",
        "\n",
        "                input_mask = torch.zeros((self.batch_size, max_len, 3), dtype=torch.long).to(device)\n",
        "                m1_mask = torch.zeros((self.batch_size, max_len, max_len, 2), dtype=torch.float32).to(device)\n",
        "                for mask_len in range(1, person_traj_len[0] + 1):  # from 1 -> len\n",
        "                    # if mask_len != person_traj_len[0]:\n",
        "                    #     continue\n",
        "                    input_mask[:, :mask_len] = 1.\n",
        "                    m1_mask[:, :mask_len, :mask_len] = 1.\n",
        "\n",
        "                    train_input = person_input * input_mask\n",
        "                    train_m1 = person_m1 * m1_mask\n",
        "                    train_m2t = person_m2t[:, mask_len - 1]\n",
        "                    train_label = person_label[:, mask_len - 1]  # (N)\n",
        "                    train_len = torch.zeros(size=(self.batch_size,), dtype=torch.long).to(device) + mask_len\n",
        "\n",
        "                    prob = self.model(train_input, train_m1, self.mat2s, train_m2t, train_len)  # (N, L)\n",
        "\n",
        "                    if mask_len <= person_traj_len[0] - 2:  # only training\n",
        "                        continue\n",
        "\n",
        "                    elif mask_len == person_traj_len[0] - 1:  # only validation\n",
        "                        acc_valid = calculate_acc(prob, train_label)\n",
        "                        cum_valid += calculate_acc(prob, train_label)\n",
        "\n",
        "                    elif mask_len == person_traj_len[0]:  # only test\n",
        "                        acc_test = calculate_acc(prob, train_label)\n",
        "                        cum_test += calculate_acc(prob, train_label)\n",
        "\n",
        "                print(step, acc_valid, acc_test)\n",
        "\n",
        "                if acc_valid.sum() == 0 and acc_test.sum() == 0:\n",
        "                    user_ids.append(step)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':#We do our work based on STAN's work\n",
        "    # load data\n",
        "    dname = 'NYC'\n",
        "    file = open('/content/drive/MyDrive/STTF-Rec/data/' + dname + '_data.pkl', 'rb')\n",
        "    file_data = joblib.load(file)\n",
        "    # tensor(NUM, M, 3), np(NUM, M, M, 2), np(L, L), np(NUM, M, M), tensor(NUM, M), np(NUM)\n",
        "    [trajs, mat1, mat2s, mat2t, labels, lens, u_max, l_max] = file_data\n",
        "    mat1, mat2s, mat2t, lens = torch.FloatTensor(mat1), torch.FloatTensor(mat2s).to(device), \\\n",
        "                               torch.FloatTensor(mat2t), torch.LongTensor(lens)\n",
        "\n",
        "    # the run speed is very flow due to the use of location matrix (also huge memory cost)\n",
        "    # please use a partition of the data (recommended)\n",
        "    part = 100\n",
        "    trajs, mat1, mat2t, labels, lens = \\\n",
        "        trajs[:part], mat1[:part], mat2t[:part], labels[:part], lens[:part]\n",
        "\n",
        "    ex = mat1[:, :, :, 0].max(), mat1[:, :, :, 0].min(), mat1[:, :, :, 1].max(), mat1[:, :, :, 1].min()\n",
        "\n",
        "    stan = Model(t_dim=hours+1, l_dim=l_max+1, u_dim=u_max+1, embed_dim=256, ex=ex, dropout=0)\n",
        "    num_params = 0\n",
        "\n",
        "    for name in stan.state_dict():\n",
        "        print(name)\n",
        "\n",
        "    for param in stan.parameters():\n",
        "        num_params += param.numel()\n",
        "    print('num of params', num_params)\n",
        "\n",
        "    load = False\n",
        "\n",
        "    if load:\n",
        "        checkpoint = torch.load('best_stan_win_' + dname + '.pth')\n",
        "        stan.load_state_dict(checkpoint['state_dict'])\n",
        "        start = time.time() - checkpoint['time']\n",
        "        records = checkpoint['records']\n",
        "    else:\n",
        "        records = {'epoch': [], 'acc_valid': [], 'acc_test': []}\n",
        "        start = time.time()\n",
        "\n",
        "    trainer = Trainer(stan, records)\n",
        "    trainer.train()\n",
        "    # trainer.inference()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq6oSiseK1eB",
        "outputId": "610f2d7d-d715-4abd-9c02-e25cfb1433a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiEmbed.emb_t.weight\n",
            "MultiEmbed.emb_l.weight\n",
            "MultiEmbed.emb_u.weight\n",
            "MultiEmbed.emb_su.weight\n",
            "MultiEmbed.emb_sl.weight\n",
            "MultiEmbed.emb_tu.weight\n",
            "MultiEmbed.emb_tl.weight\n",
            "transformer_blocks.0.attention.linear_layers.0.weight\n",
            "transformer_blocks.0.attention.linear_layers.0.bias\n",
            "transformer_blocks.0.attention.linear_layers.1.weight\n",
            "transformer_blocks.0.attention.linear_layers.1.bias\n",
            "transformer_blocks.0.attention.linear_layers.2.weight\n",
            "transformer_blocks.0.attention.linear_layers.2.bias\n",
            "transformer_blocks.0.attention.output_linear.weight\n",
            "transformer_blocks.0.attention.output_linear.bias\n",
            "transformer_blocks.0.feed_forward.w_1.weight\n",
            "transformer_blocks.0.feed_forward.w_1.bias\n",
            "transformer_blocks.0.feed_forward.w_2.weight\n",
            "transformer_blocks.0.feed_forward.w_2.bias\n",
            "transformer_blocks.0.input_sublayer.norm.a_2\n",
            "transformer_blocks.0.input_sublayer.norm.b_2\n",
            "transformer_blocks.0.output_sublayer.norm.a_2\n",
            "transformer_blocks.0.output_sublayer.norm.b_2\n",
            "AttnMatch.value.weight\n",
            "AttnMatch.emb_loc.weight\n",
            "num of params 2427492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:21<00:00,  1.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1, time:83.42453384399414, valid_acc:[0.15 0.3  0.36 0.42]\n",
            "epoch:1, time:83.42648410797119, test_acc:[0.11 0.23 0.31 0.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:2, time:160.86049795150757, valid_acc:[0.22 0.44 0.5  0.54]\n",
            "epoch:2, time:160.86194014549255, test_acc:[0.18 0.36 0.4  0.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:17<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:3, time:238.06651091575623, valid_acc:[0.18 0.47 0.53 0.56]\n",
            "epoch:3, time:238.0680341720581, test_acc:[0.16 0.3  0.39 0.41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:4, time:315.53729367256165, valid_acc:[0.18 0.38 0.47 0.55]\n",
            "epoch:4, time:315.53897356987, test_acc:[0.13 0.29 0.35 0.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:17<00:00,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:5, time:392.96579933166504, valid_acc:[0.21 0.39 0.47 0.51]\n",
            "epoch:5, time:392.96696186065674, test_acc:[0.12 0.3  0.39 0.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}